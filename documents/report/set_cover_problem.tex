%!TEX program = lualatex
\documentclass[12pt,letterpaper,twoside]{article}
\include{packages}
\include{settings}

\bibliography{references}
\nocite{*}

\begin{document}
	\maketitle{}
	{\footnotesize
	\tableofcontents{}}
	\newpage{}
	\section{Définition du problème}
		\subsection{SCP et WSCP}
			\paragraph*{Présentation\\}
				Le problème de couverture d'ensemble, ou \emph{Set Covering Problem} (SCP),
				fait parti des 21 problèmes NP-complets de \citeauthor{Karp1972}~\cite{Karp1972}
				et est NP-complet au sens fort \cite{garey2002computers}.
			\paragraph*{Problème de couverture d'ensemble\\}
				Étant donné un ensemble univers \(U = \{u_1, u_2, u_3, \dots, u_m\}\) et une famille \(S = \{s_1, s_2, \dots, s_n\}\) de sous-ensembles de \(U\),
				le problème consiste à trouver une sous-famille de \(S\) la plus petite possible permettant de couvrir chaque élément de \(U\)
				au moins une fois. Un élément \(e\) de \(U\) est couvert par un sous-ensemble \(A\) si \(e \in A\).
			\paragraph*{Problème de couverture d'ensemble pondéré\\}
				En associant un coût positif \(c_i\) à chaque sous-ensemble, on obtient le problème de couverture d'ensemble pondéré ou \emph{Weighted Set Covering Problem} (WSCP) et	l'objectif est alors de déterminer une couverture de coût minimum.~\cite{Vazirani2003}
		\subsection{Utilité}
			\paragraph*{}
				Une grande variété de problèmes de positionnement, de distribution, de planification et autres peuvent être formulés comme variantes du problème de couverture d'ensemble. Parmi les problèmes réels auxquels cette approche a été appliquée avec succès:\cite{Balas1982}
				\begin{itemize}
					\item problèmes de sélection de sites et d'allocation d'emplacement
					\item emplacement des installations des services d'urgence (casernes de pompiers, hôpitaux, etc.)
					\item choix de la taille et de l'emplacement des plates-formes de forage dans les champs pétrolifères en mer
					\item horaire des équipages pour les compagnies aériennes, les compagnies de bus, les chemins de fer
					\item répartition des fréquences de radiodiffusion entre stations de radio ou de télévision
					\item recherche d'informations (à partir de fichiers informatiques)
					\item \ldots
				\end{itemize}
	\newpage\section{Exemple minimal}
		\paragraph*{}
			Soit un ensemble univers \(U = \{u_1, u_2, \dots, u_{12}\}\) (représenté par des points sur le figure \ref{fig:example}) et une famille \(S = \{s_1, s_2, \dots, s_6\}\) de sous-ensembles de \(U\) (représentés par des rectangles sur la figure \ref{fig:example}) avec:
			\begin{itemize}
				\item \(s_1 = \{u_1, u_2, u_3, u_4, u_5, u_6\}\)
				\item \(s_2 = \{u_5, u_6, u_8, u_9\}\)
				\item \(s_3 = \{u_1, u_4, u_7, u_{10}\}\)
				\item \(s_4 = \{u_2, u_5, u_8, u_{11}\}\)
				\item \(s_5 = \{u_3, u_6, u_9, u_{12}\}\)
				\item \(s_6 = \{u_{10}, u_{11}, u_{12}\}\)
			\end{itemize}
		\paragraph*{}
			La solution optimale à cette instance est la sous famille \(S'=\{s_3, s_4, s_5\}\) (colorée en gris sur la figure \ref{fig:example}).
		\begin{figure}[H]
			\centering%
			\includegraphics[width=0.65\linewidth]{figures/example}%
			\caption{Exemple d'instance du Set Cover Problem et solution optimale\cite{Mount2017}}%
			\label{fig:example}%
		\end{figure}
	\section{Complexité}
		\paragraph*{}
			Le SCP est un problème d'optimisation NP-difficile, et NP-complet dans sa forme décisionnelle. Il fait notamment parti des 21 problèmes NP-complets de \citeauthor{Karp1972}~\cite{Karp1972} et est NP-complet au sens fort~\cite{garey2002computers}.
		\paragraph*{}
			La démonstration de la NP-complétude du problème a été réalisée par \citeauthor{Karp1972} en \citeyear{Karp1972} dans son article \citetitle{Karp1972}\cite{Karp1972}. Dans cet article, il réalise des réductions pour 21 problèmes réputés difficiles de combinatoire et de théorie des graphes comme représenté sur la figure \ref{fig:karp_reduction_tree}.
		\begin{figure}[H]
			\centering%
			\includegraphics[width=\linewidth]{karp_reduction_tree}%
			\caption{Arbre des réductions réalisées par Karp\cite{Karp1972}}%
			\label{fig:karp_reduction_tree}%
		\end{figure}
		\paragraph*{}
			Concernant notre problème, \citeauthor{Karp1972} montre donc que le Boolean Satisfiability Problem (SATISFIABILITY) peut être réduit au Clique Problem (CLIQUE) qui peut etre réduit au Vertex/Node Cover Problem (NODE COVER) qui peut être réduit au Set Covering Problem (SET COVERING).
		\paragraph*{}
			Le théorème de Cook–Levin et sa démonstration publiée en \citeyear{Cook1971} par \citeauthor{Cook1971} dans l'article \citetitle{Cook1971}\cite{Cook1971} prouve le Boolean Satisfiability Problem comme étant un problème NP-complet. Par réduction, le Set Covering Problem est donc aussi NP-complet.
		\paragraph*{}
			Pour ce qui est du Weighted Set Covering Problem (WSCP), c'est une généralisation du SCP et la réduction de ce dernier est évidente, il suffit de rajouter des poids tous égaux a une instance de SCP pour obtenir une instance de WSCP équivalente, le WSCP est donc lui aussi NP-complet.
	\section{État de l'art}
		\paragraph*{}
			Les algorithmes présentés dans cette section utilisent souvent le modèle de la relaxation lagrangienne.
			Cette relaxation consiste à supprimer des contraintes difficiles en les intégrant dans la fonction objectif en tant que pénalités.
			Les coefficients appliqués sont dans ce qu'on appelle le « vecteur multiplicateur de lagrange ».~\cite{fisher1985applications}
		\subsection{Méthodes exactes}
			\paragraph*{}
				La plupart des méthodes exactes les plus efficaces pour résoudre le problème de couverture d'ensemble sont
				des méthodes branch-and-bound
				dans lesquelles on calcule les bornes inférieures à l'aide d'une relaxation du problème en programmation
				linéaire.~\cite{caprara2000algorithms}
			\paragraph*{}
				\citeauthor{Beasley1987} a proposé un algorithme qui fonctionne de cette façon.
				Le nœud racine utilise la solution du programme linéaire relaxé, puis les bornes inférieures
				sont calculées à l'aide de la relaxation lagrangienne avec la descente de sous-gradient.~\cite{Beasley1987}
		\subsection{Méthodes approchées}
			\subsubsection{Méthodes voraces non optimales}\label{sec:soa-greedy}
				\paragraph*{}
					\citeauthor{Johnson:1973:AAC:800125.804034} a présenté dans son papier \citetitle{Johnson:1973:AAC:800125.804034}
					l'algorithme vorace standard pour le problème de couverture d'ensemble non pondéré.
					L'algorithme prend simplement le sous-ensemble qui couvre le plus
					d'éléments à chaque étape jusqu'à obtenir une solution valide. La complexité de l'algorithme est en \(O(mn)\) avec \(m\)
					le nombre d'éléments et \(n\) le nombre de sous-ensembles.~\cite{Johnson:1973:AAC:800125.804034}
					Cet algorithme n'est pas très efficace sur un problème de couverture d'ensemble pondéré car il n'en tient pas du
					tout compte.
				\paragraph*{}
					\citeauthor{Cormen:2009:IAT:1614191} ont présenté une version de l'algorithme utilisant une queue à priorité,
					permettant d'améliorer la vitesse de l'algorithme.~\cite{Cormen:2009:IAT:1614191}
				\paragraph*{}
					\citeauthor{Goldschmidt:1993:MGH:177276.177287} ont proposé une solution qui fonctionne en appliquant des
					combinaisons de plusieurs algorithmes. Des algorithmes approchés et exacts sont utilisés.
					La méthode exploite le fait qu'il est possible de trouver la solution optimale pour des ensembles de taille 2 au maximum
					en un temps polynomial. On exécute l'algorithme vorace standard de Johnson jusqu'à obtenir une taille maximale de 2 pour les
					ensembles, puis trouve la couverture optimale pour le reste, ce qui donne une solution améliorée.~\cite{Goldschmidt:1993:MGH:177276.177287}
			\subsubsection{Heuristiques}
				\paragraph*{}
					De nombreuses méthodes par heuristique sont basées sur l'observation que pour un vecteur multiplicateur de lagrange quasi-optimal
					le coût de lagrange donne une information fiable sur l'utilité générale de la sélection d'un sous-ensemble donné.
					De là, plutôt que d'utiliser les coûts originaux, on utilise les coûts de lagrange pour calculer les scores permettant
					de classer les différents sous-ensembles à sélectionner pour la solution optimale. Ces scores sont donnés
					à un algorithme vorace pour obtenir une solution valide.~\cite{caprara2000algorithms}
				\paragraph*{}
					\citeauthor{Beasley1990} a proposé un algorithme qui calcule à chaque itération de l'optimisation par sous-gradient une
					solution valide de la façon suivante : un ensemble \(S\) est initialisé avec les sous-ensembles sélectionnés
					par la solution de la relaxation lagrangienne. Puis, pour chaque point non couvert par \(S\),
					le sous ensemble avec le coût original le plus faible est ajouté à \(S\). Pour finir, les différents sous-ensembles
					de \(S\) sont considérés par ordre de coût original décroissant et on enlève le sous-ensemble \(j\) si \(S \backslash \{j\}\)
					est toujours une solution valide. À chaque itération on effectue un fixage de coût de lagrange pour réduire la taille
					du problème.~\cite{Beasley1990}	
				\paragraph*{}
					\citeauthor{jacobs1995lsh} ont proposée une approche basée sur le recuit simulé. Une solution initiale \(S\)
					est générée par un algorithme vorace qui à chaque itération sélectionne aléatoirement un point non couvert
					et ajoute à la solution le premier sous-ensemble qui couvre ce point. Après cette addition, les colonnes
					redondantes sont enlevées de la solution, et le processus est continué jusqu'à l'obtention d'une solution valide.
					Ensuite, on effectue un certain nombre d'itérations de recuit simulé : on choisi un sous-ensemble dans \(S\) aléatoirement
					et on l'enlève, puis on complète de manière vorace de manière à obtenir une autre solution valide \(S'\). Si \(S'\)
					est une meilleure solution que \(S\), \(S'\) remplace \(S\), sinon il remplace avec une certaine probabilité qui
					décroit exponentiellement au fil des itérations.~\cite{jacobs1995lsh}
				\paragraph*{}
					\citeauthor{Brusco1999} ont ensuite proposé une amélioration à cette méthode.
					Premièrement, au lieu de choisir le sous-ensemble à enlever aléatoirement à chaque itération, on ne le fait plus
					que toutes les trois itérations. Pendant les deux itérations restantes, on choisi simplement le sous-ensemble qui
					laisse le moins de point non couvert possible. Deuxièmement, pour chaque sous-ensemble \(j\) de la solution,
					on utilise une liste de « morphs », qui sont tout simplement les autres sous-ensembles « similaires » à \(j\).
					Après avoir obtenu la solution partielle et tous les quatre sous-ennsemble ajouté pour la compléter, on itère
					sur tous les sous-ensembles de la solution partielle courante
					et on remplace chaque sous ensemble par l'un de ses « morphs » si cela améliore le ratio entre le coût actuel
					et le nombre de point couverts.~\cite{Brusco1999}
				\paragraph*{}
					\citeauthor{Afif1995} ont présenté une solution basée sur l'algorithme du flot de Ford-Fulkerson qui est un algorithme
					en temps polynomial.
					Il s'agit d'effectuer une transformation du problème en un graphe de flot à résoudre par une variante de l'algorithme
					de Ford-Fulkerson. Il est montré que le problème de couverture d'ensemble peut se réduire à un problème de flot minimal
					en temps polynomial.~\cite{Afif1995}
	\section{Représentation du problème}
		\subsection{Représentation mathématique}
			\paragraph*{}
				On reprend la notation:
				\begin{itemize}
					\item \(U = \{u_1, u_2, u_3, \dots, u_m\}\), ensemble univers composé de \(m\) points
					\item \(S = \{s_1, s_2, \dots, s_n\}\), famille de \(n\) sous-ensembles de \(U\)
				\end{itemize}
				et on pose \(M = \{1,\ldots,m\}\) et \(N = \{1,\ldots,n\}\).
			\paragraph*{Matrice d'incidence\\}
				On définit la matrice d'incidence \(A = \left(a_{i,j}\right)\) de taille \(m \times n\) avec
				\[\forall i \in M,\ \forall j \in N,\ a_{i,j} = \left\{
				    \begin{array}{ll}
				        1 & \text{si } u_i \in s_j \\
				        0 & \text{sinon}
				    \end{array}
				\right.\]
				\(a_{i,j} = 1\) signifiant donc que le point \(i\) est couvert par le sous-ensemble \(j\).
			\paragraph*{Vecteur coût\\}
				On définit le vecteur coût \(n\)-dimensionnel \(c = \left(c_j\right)\) avec \(\forall j \in N\), \(c_j\) le coût du sous ensemble \(j\).
			\paragraph*{Solution\\}
				On définit un vecteur solution comme un vecteur \(n\)-dimensionnel \(x = \left(x_j\right)\) avec
				\[\forall j \in N,\ x_j = \left\{
				    \begin{array}{ll}
				        1 & \text{si } u_i \text{ fait parti de la solution}\\
				        0 & \text{sinon}
				    \end{array}
				\right.\]
				La solution ayant comme coût \(\sum_{j \in N}{c_i x_i}\) et étant valide si
				\[\forall i \in M\ ,\sum_{j \in N}{a_{ij}x_i} \ge 1\]
		\subsection{Représentation dans notre projet}
			\paragraph*{}
				La première chose que l'on peut remarquer dans la représentation de notre problème, est que la matrice d'incidence \(A\) ainsi que le vecteur solution \(x\) ne contiennent que des valeurs booléennes (égales \(0\) ou \(1\)).
			\paragraph*{}
				La deuxième chose, qui est assez commune dans le champ de l'optimisation, et particulièrement vrai dans le cas des algorithmes que nous avons implémenté, est qu'une grande partie du temps de calcul est dédié a la vérification de la validité des solutions ainsi qu'a l'évaluation des solution par la fonction objectif.
			\paragraph*{}
				Notre fonction objective étant
				\[f(x) = \sum_{j \in N}{c_i x_i}\]
				Cette dernière peut être implémentée avec une boucle simple et sa complexité est en \(\mathcal{O}(n)\).
			\paragraph*{}
				Pour ce qui est de la vérification de la validité des solution, il faut s'assurer que:
				\[\forall i \in M\ ,\sum_{j \in N}{a_{ij}x_i} \ge 1\]
				Un algorithme évident est de faire deux boucles imbriquées, la première de longueur \(m\) sur les points et la deuxième de longueur \(n\) sur les sous-ensembles. Cet algorithme aurait alors une complexité en \(\mathcal{O}(mn)\). Une première optimisation serait d’arrêter la deuxième boucle au premier sous-ensemble qui couvre le point, cependant nous avons plutôt décider d'adopter une approche qui permet de prendre avantage des processeurs modernes (post-2013).
			\paragraph*{}
				En effet les processeurs modernes sont équipés d'instruction SIMD (Single Instruction Multiple Data), on peut notamment penser à l'AVX512 sur les processeurs Intel qui permet de réaliser des opérations sur 512 bits simultanément. Nous avons donc décidé de stocker la matrice d'incidence \(A\) ainsi que le vecteur solution \(x\) sur les bits de types primitifs \Cpp{} afin de pouvoir leur appliquer des opérations simultanément. Nous allons considérer pour les paragraphes suivants un processeur 64 bits standard et le fait que les types primitifs courants sont sur 64 bits (ce qui est une simplification ne prenant pas en compte les possibles optimisations que le compilateur pourrait réaliser).
			\paragraph*{}
				Pour cela nous utilisons une classe \texttt{dynamic\_bitset}\cite{dynamicbitset}, implémentée par un membre du groupe, permettant d'utiliser un bitset dynamique (dont la taille n'est pas connue a la compilation) et d'appliquer toutes les opérations booléennes de façon optimisée en tirant avantage des instructions SIMD du processeur si elles sont présentes. De plus les processeur standards sont aussi équipés d'instructions pour connaitre le nombre de bits à \(1\) et la position du bit de poids le plus faible à \(1\). Cela divise donc au mieux par 64 la complexité du parcourt de \(x\) pour trouver le prochain sous-ensemble inclut dans la solution. Et enfin le fait de stocker \(k\) informations sur \(k\) bits au lieu de \(64k\), permet de faire une bien meilleure utilisation du cache processeur et nous permet de gagner énormément en performances et d'espace mémoire.
			\paragraph*{}
				La matrice d'incidence \(A\) devient donc un vecteur de \(n\) bitsets de taille \(m\), un par sous-ensemble et le vecteur solution \(x\) devient un bitset de taille \(n\). La figure \ref{fig:representation_to_bitsets} illustre ce passage pour une instance a 5 points et 7 sous-ensembles.
			\begin{figure}[H]
				\centering%
				\resizebox{0.8\textwidth}{!}{\includestandalone{./figures/representation_to_bitsets}}%
				\caption{Passage de la représentation mathématique aux bitsets (instance a 5 points et 7 sous-ensembles)}%
				\label{fig:representation_to_bitsets}%
			\end{figure}
			\paragraph*{}
				Avec une telle représentation, la vérification de la validité d'une solution peut se faire efficacement en réalisant une opération booléenne \textit{OR} sur tous les bitsets \(S_i\) pour lesquels \(x_1\) est à \(1\) et en vérifiant que le bitset résultat de cette opération a tous ses bits à \(1\). La figure \ref{fig:solution_validity_check_function} représente cette opération.
			\begin{figure}[H]
				\centering%
				\resizebox{\textwidth}{!}{\includestandalone{./figures/solution_validity_check_function}}%
				\caption{Vérification de la validité d'une solution avec la représentation à base de bitsets (instance a 5 points et 7 sous-ensembles)}%
				\label{fig:solution_validity_check_function}%
			\end{figure}
			\paragraph*{}
				L'opération \textit{OR} sur les bitsets ainsi que la vérification que tous les bits sont à \(1\) est de complexité \(\mathcal{O}(1)\) pour un nombre de bits raisonnable (ce qui est largement le cas même pour les plus grandes instances que nous utiliserons). Notre algorithme de vérification de la validité d'une solution est donc en \(\mathcal{O}(n)\).
			\paragraph*{}
				La complexité des opérations sur notre problème étant donc toutes en \(\mathcal{O}(n)\), on ne s'occupera plus par la suite du nombre de points \(m\) et on dira qu'un problème avec \(m\) points et \(n\) sous-ensembles est un problème de taille \(n\).
	\section{Instances du problème}
		\subsection{OR-Library}
			\paragraph*{}
				On utilise les groupes d'instances mis a disposition par \citeauthor{OR-Library} dans son regroupement d'instances OR-Library\cite{OR-Library}. Parmis ces instances, celles de 4 à 6 proviennent l'article \citetitle{Balas1980}\cite{Balas1980} de \citeauthor{Balas1980}, celles de A à D proviennent de l'article \citetitle{Beasley1987}\cite{Beasley1987} de \citeauthor{Beasley1987} et celles de E à H proviennent de l'article \citetitle{Beasley1990}\cite{Beasley1990} de \citeauthor{Beasley1990}.
			\paragraph*{}
				Toutes les instances du problème de ces groupes ont été générées en utilisant le shémas de \citeauthor{Balas1980}\cite{Balas1980} dans lequel le coût \(c_i\) de chaque colonne \(i\) est pris aléatoirement dans l'intervalle \(\llbracket0,100\rrbracket\), chaque colonne couvre au moins une ligne et chaque ligne est couverte par au moins deux colonnes.
			\paragraph*{}
			   Les propriétés de ces groupes d'instances sont décrites dans la table \ref{table:scp_problem_sets}, la densitée étant la proportion de \(1\) dans la matrice \(a_{i,j}\). La table \ref{table:orlibrary_scp_problems_optimal_solutions}, contient les valeurs optimales pour les problèmes pour lesquels elle est connue.
			\begin{table}[H]
				\centering
				\input{tables/orlibrary_scp_problem_sets}
				\caption{Groupes d'instances du SCP utilisées\cite{OR-Library,Balas1980,Beasley1987,Beasley1990}}
				\label{table:scp_problem_sets}
			\end{table}
			\begin{table}[H]
				\centering
				\begin{minipage}[t]{0.45\linewidth}
					\centering
					\input{tables/orlibrary_scp_problems_optimal_solutions_1}
				\end{minipage}
				\begin{minipage}[t]{0.45\linewidth}
					\centering
					\input{tables/orlibrary_scp_problems_optimal_solutions_2}
				\end{minipage}
				\caption{Solutions optimales des instances du SCP utilisée\cite{Beasley1990}}
				\label{table:orlibrary_scp_problems_optimal_solutions}
			\end{table}
		\subsection{Instances générées}\label{sec:generated-instances}
			\paragraph*{}
				Les instances de OR-Library étant de grande taille, elles ne nous permettent pas de tester nos méthodes exactes, nous avons donc également développé un générateur d'instances utilisant la même méthode que pour les instances d'OR-Library (schémas de \citeauthor{Balas1980}\cite{Balas1980}).
			\paragraph*{}
				Afin de benchmarcker les différentes méthodes implémentées, notamment les méthodes exactes avec des instances identiques pour avoir des valeures comparables, nous avons utilisé le générateur pour réaliser un ensemble d'instances de test de taille \(2\) a \(100\) avec \(200\) points.
			\paragraph*{}
				La table \ref{table:generated_scp_problems_optimal_solutions} regroupe les solutions optimales de nos instances lorsque les méthodes exactes nous ont permis de les obtenir.
			\paragraph*{}
			\begin{table}[H]
				\centering
				\begin{minipage}[t]{0.3\linewidth}
					\centering
					\input{tables/generated_scp_problems_optimal_solutions_1}
				\end{minipage}
				\begin{minipage}[t]{0.3\linewidth}
					\centering
					\input{tables/generated_scp_problems_optimal_solutions_2}
				\end{minipage}
				\begin{minipage}[t]{0.3\linewidth}
					\centering
					\input{tables/generated_scp_problems_optimal_solutions_3}
				\end{minipage}
				\caption{Solutions optimales des instances générées}
				\label{table:generated_scp_problems_optimal_solutions}
			\end{table}
	\section{Méthodes exactes}
		\subsection{Recherche exhaustive}
			\paragraph*{}
				La méthode exacte la plus simple est encore la recherche exhaustive, dans notre cas, cela consiste à tester toutes les valeurs possibles du vecteur de solution \(x\). Ce vecteur étant un vecteur booléen, il y a \(2^n\) possibilités pour un vecteur de taille \(n\) qui correspond à un problème de taille \(n\).
			\paragraph*{}
				Pour le SCP simple, il n'est pas nécessaire de tester toutes les permutations possibles, il suffit de commencer par les permutations de 1 bit, puis 2,3\ldots et dès que la permutation est une solution valide, cette dernière est la solution optimale puisque la première valide en le moins de sous-ensembles possibles.
			\paragraph*{}
				Nous avons commencé par utiliser les algorithmes classiques connu qui sont récursifs, mais ces derniers atteignent très rapidement la limite de stack du programme de par le nombre élevé d'appels récursifs pour les taille de bitset dont nous avons besoin. Ils ne sont donc pas envisageables pour notre problème.
			\paragraph*{}
				Nous avons donc réalisé un générateur de permutations G1 qui utilise une méthode s'inspirant de la programmation dynamique. En effet les permutations de \(k\) bits parmi \(n\) sont les permutations de \(k\) bits parmi \(n-1\) avec le \(n\)-ème bit a \(0\) ajouté aux permutations de \(k-1\) bits parmi \(n-1\) avec le \(n\)-ème bit a \(1\). En notant \(P_k^n\) les permutations de \(k\) bits parmi \(n\), on peut donc généré facilement \(P_k^n\) a l'aide de \(P_k^{n-1}\) et de \(P_{k-1}^{n-1}\).
			\paragraph*{}
				Le générateur G1 génère donc les \(P_i^n\) pour \(i \in \llbracket 0, n \rrbracket\) en générant successivement les \(P_i^t\) pour \(t\in \llbracket 1, n \rrbracket\) en utilisant les \(P_i^t\) de la génération précédente comme représenté sur la figure \ref{fig:g1_permutations}.
			\begin{figure}[H]
				\centering%
				\resizebox{0.4\textwidth}{!}{\includestandalone{./figures/g1_permutations}}%
				\caption{Méthode de génération des \(\forall i \in \llbracket 0, n \rrbracket,\ P_i^n\) du générateur G1}%
				\label{fig:g1_permutations}%
			\end{figure}
			\paragraph*{}
				Le problème avec générateur G1 est qu'il a besoin de stocker toutes les permutations de la génération précédente pour générer la suivante or le nombre de permutations augmente en \(2^n\) et son utilisation de RAM pour stocker les permutations dépasse les 8Go pour générer les permutations sur un bitset de taille 27 (voir figure \ref{figure:permutations_generators_ram}).
			\begin{figure}[H]
				\centering
				\input{plots/permutations_generators_ram}%
				\caption{Utilisation de la RAM par les générateurs de permutations}
				\label{figure:permutations_generators_ram}
			\end{figure}
			\paragraph*{}
				Nous avons donc réalisé un générateur G2 qui ne nécessite pas de stocker les permutation des génération précédentes mais va les construires dynamiquement au fil de la génération des \(P_i^n\), ce générateur est donc bien moins performant (voir figure \ref{figure:plots/permutations_generators_time}) mais a l'avantage de ne pas consommer excessivement de RAM (voir figure \ref{figure:permutations_generators_ram}). Le générateur G2 peut générer les \(P_i^n\) jusqu'à \(n = 32\) en moins d'une heure, ce qui est bien loin d’être suffisant pour des grandes instances tels que celles de OR-Library.
			\paragraph*{}
				Les générateurs G1 et G2 génèrent les permutations de 1 bit, puis 2,3\ldots afin d’être efficace pour le SCP simple mais il est possible de générer ces permutations de façon plus efficace si on ne restreint pas l'ordre de génération. Nous avons donc codé un générateur G3 qui est plus performant que G1 et G2 mais ne garantit pas un ordre croissant sur le nombre de bits a \(1\). G3 est donc moins performant que G1 et G2 pour le SCP mais plus performant pour le WSCP.
			\paragraph*{}
				Le générateur G3 génère les permutations en considérant le bitset comme la représentation binaire d'un nombre, il suffit de commencer a 0 et d'incrémenter le nombre représenté jusqu'à retourner a 0 pour être passé par toutes les valeurs représentables sur \(n\) bits et donc toutes les permutations. L'algorithme consiste donc à appliquer successivement l'algorithme d'incrément binaire qui commence par le bit de poids le plus faible et remonte les bits en transformant les \(1\) en \(0\) jusqu’à arriver à la fin du bitset ou à un \(0\) qu'il transforme en \(1\).
			\paragraph*{}
				Les 3 générateurs réalisent (en pire cas pour G3) \(\mathcal{O}(n)\) opérations pour générer chaque permutation, la complexité des 3 générateurs pour générer les \(2^n\) permutations est donc en \(\mathcal{O}(n2^n)\). Pour les comparer, on réalise donc une étude empirique dont les résultats sont visibles sur les graphiques de la figure \ref{figure:plots/permutations_generators_time}. Comme prévu, le générateur G3 est le plus performant, cependant il n'est pas adapté pour le SCP simple.
			\begin{figure}[H]
				\centering
				\input{plots/permutations_generators_time_lin}\\
				\input{plots/permutations_generators_time_log}%
				\caption{Temps nécessaire pour générer toutes les permutations d'un bitset}
				\label{figure:plots/permutations_generators_time}
			\end{figure}
			\paragraph*{}
				En se fiant aux données de notre étude, sur l'ordinateur de test, la durée nécessaire au générateur G3 pour générer les permutations d'un vecteur de taille \(x\) est approximable par la fonction \(4.114961289.10^{-9}\left(e^{0.693072216x} + 1\right)\). Si l'on voulait résoudre les plus petites instances de OR-Library à \(1000\) éléments, il faudrait donc un temps théorique de \(4 \times 10^{492}\)s au générateur G3 pour générer les permutations du vecteur \(x\) et tester toutes les possibilités. Au vu de l'inefficacité de la recherche exhaustive nous avons décidé d'implémenter une seconde méthode exacte basée sur un Branch-and-Bound.
		\subsection{Branch-and-Bound}\label{sec:bnb}
			\paragraph*{}
				Dans l'article \citetitle{caprara2000algorithms} de \citeauthor{caprara2000algorithms} on peut lire ceci:
				\begin{quote}
					``The most effective exact approaches to SCP are branch-and-bound algorithms in which lower bounds are computed by solving the LP relaxation of SCP[\ldots]. In particular, all the [exact] algorithms which have been tested on the instances from the OR Library are of this type. The main reason for the success of these approaches is the fact that, despite the LP lower bound is not always very strong for these instances, it is apparently very difficult to get significantly stronger lower bounds by alternative methods which are computationally more expensive.''~\cite{caprara2000algorithms}
				\end{quote}
			\paragraph*{}
				Le Branch-and-Bound est donc une méthode efficace pour notre problème mais requiert un solveur de programme linéaire dans son application la plus performante. Développer un tel solveur ou l'intégrer au projet d'une source externe pouvant prendre un temps considérable et la tâche s'éloignant de notre but de travailler sur des méthodes spécifiques au WSCP, nous avons décidé d'implémenter un Branch-and-Bound spécifique au problème sans solveur de programme linéaire.
			\paragraph*{}
				Notre Branch-and-Bound parcourt donc un arbre de décision où, au niveau \(i\), le choix de la valeur du bit \(i\) (\(0\) ou \(1\)) crée une séparation en 2 branches tel que représenté sur le figure \ref{fig:bnb}. Les bits dont la valeur n'a pas encore été décidée sont a \(0\) afin que les sous-ensembles auxquels ils correspondent n'interviennent pas dans le coût de la solution.
			\paragraph*{}
			\begin{figure}[H]
				\centering%
				\resizebox{0.75\textwidth}{!}{\includestandalone{./figures/bnb}}%
				\caption{Arbre parcourut par la Branch-and-Bound (ici sans coupures) pour un bitset de taille 3}%
				\label{fig:bnb}%
			\end{figure}
			\paragraph*{}
				La borne utilisée pour couper des branches de l'arbre est une borne maximum sur le coût de la solution. Cette borne est initialisée avec le coût de la solution trouvée par l'algorithme vorace pondéré (weighted greedy) qui sera présenté dans la section \ref{sec:approx-greedy}. Cette solution initiale est aussi utilisée comme solution optimale potentielle initiale.
			\paragraph*{}
				A chaque noeud de l'arbre la solution est évaluée, si cette dernière a une évaluation plus grande que la borne, la branche est coupée, ajouter des sous-ensembles à la solution ne pouvant qu'augmenter son coût qui dépasse déjà celui de la borne. Si elle a une évaluation plus faible que la borne, si la solution n'est pas valide, l'algorithme continue, peut-être en ajoutant plus de sous-ensemble cela arrivera à une solution valide avec un coût toujours inférieur a la borne, si la solution est valide, son coût devient la nouvelle borne et la solution devient la nouvelle solution optimale potentielle.
			\paragraph*{}
				La complexité moyenne d'un Branch-and-Bound est très dure à évaluer mais on peut au moins être sûr que dans le pire cas la complexité est égale à celle de la recherche exhaustive et est donc en \(\mathcal{O}(n2^n)\). Nous avons donc réalisé une étude empirique afin d'avoir une idée de l'amélioration apportée par le Branch-and-Bound par rapport à la recherche exhaustive. Nous avons donc mesuré le temps de résolution de la recherche exhaustive avec le générateur G3 (le plus rapide) ainsi que avec le Branch-and-Bound, les résultats sont visibles sur les graphiques de la figure \ref{figure:plots/exhaustive_bnb_time}.
			\paragraph*{}
				Les résolutions ont été réalisées sur les instances de problèmes que nous avons généré (voir section \ref{sec:generated-instances}) de la plus petite à la plus grande en arrêtant la résolution au bout d'une heure si cette dernière n'était pas terminée (donnant des points de discontinuité dans les graphiques de la figure \ref{figure:plots/exhaustive_bnb_time}). C'est aussi lors de cette étude que le Branch-and-Bound nous a donné les résultats exacts présentés dans la section \ref{sec:generated-instances}, les résultats manquant étant ceux des instances pour lequel 1 heure n'a pas suffit à les résoudre.
			\paragraph*{}
				Les temps de résolution du Branch-and-Bound sont bien meilleurs que ceux de le recherche exhaustive, cependant sa performance dépend des instances et sans une configuration avantageuse, il ne nous sera toujours pas possible de résoudre les instances de OR-Library qui contiennent uniquement des problèmes de taille \(\ge 1000\). Les méthodes exactes atteignant leur limites, ils nous faut donc nous tourner du côté des méthodes approchées.
			\paragraph*{}
				\hfill{}
			\begin{figure}[H]
				\centering
				\input{plots/exhaustive_bnb_time_lin}\\
				\input{plots/exhaustive_bnb_time_log}%
				\caption{Temps nécéssaire pour résoudre un problème pour la recherche exhaustive et le Branch and bound (les résolutions trop longues ont été arrêtées après 1h d'éxécution et leurs valeurs ne sont pas représentées)}
				\label{figure:plots/exhaustive_bnb_time}
			\end{figure}
	\section{Méthodes approchées}
		\subsection{Algorithmes voraces}\label{sec:approx-greedy}
			\paragraph*{}
				Nous avons commencé par implémenter des algorithmes voraces qui permettent de trouver des solutions approchées très rapidement.
			\paragraph*{}
				Le premier algorithme que nous avons implémenté est celui de
				\citeauthor{Johnson:1973:AAC:800125.804034}~\cite{Johnson:1973:AAC:800125.804034} présenté dans la section~\ref{sec:soa-greedy}
				où l'on intègre dans la solution les sous-ensembles couvrant le plus de point d'abord.
				On utilise le nombre de points qu'un sous-ensemble permet de couvrir de façon non redondante comme score.
			\paragraph*{}
				Pour le problème \emph{A.1} dont la solution optimale est \(253\), on obtient la solution de coût \(1444\).
				Pour un problème de couverture d'ensembles pondéré, cet algorithme semble donc très mauvais.
			\paragraph*{}
				Nous avons donc décidé d'implémenter une version de l'algorithme qui prend en compte le coût des sous ensembles pour
				sélectionner lequel ajouter.
				Pour cela, nous utilisons le rapport \(\frac{n}{c}\) avec \(n\) le nombre de points actuellement non couverts que le
				sous-ensemble nous permettrait de couvrir et \(c\) le coût du sous-ensemble. C'est à dire le rapport de l'utilité sur
				le coût de la même façon que présenté dans le cours dans le chapitre sur les algorithmes voraces.
			\paragraph*{}
				De la même façon que précédememnt, pour le problème \emph{A.1} dont la solution optimale est \(253\), on obtient cette fois
				une solution de coût \(288\).

			\paragraph*{}
				On voit donc qu'utiliser le ratio utilité / coût nous permet donc d'obtenir une solution bien meilleure.
				On constate que dans le cas du problème de couverture d'ensemble pondéré, la meilleure solution n'est pas
				toujours celle qui utilise le moins de sous-ensembles. En effet, cette dernière solution utilise \(89\) solutions contre \(42\).

			\paragraph*{}
				Nous avons comparé plus en profondeur en utilisant les solutions optimales calculées dans la Section~\ref{sec:bnb}.
				Pour cela, on utilise le ratio de différence \(\frac{r - opt}{opt}\) avec \(r\) le résultat obtenu et \(opt\) la
				valeur optimale.
				La Figure~\ref{fig:plots/greedy-diff} est un graphique représentant ce ratio pour chacun des problèmes
				dont nous avons réussi à calculer la solution optimale.

			\begin{figure}[H]
				\centering
				\input{plots/greedy_diff.tex}\\
				\caption{}
				\label{fig:plots/greedy-diff}
			\end{figure}

			\paragraph*{}
				On voit que sur un problème de petite taille, il peut arriver que l'heuristique ne prenant pas en compte le poids
				des ensembles trouve une meilleure solution (voire parfois la solution optimale elle-même), cependant
				sur des problèmes de taille plus grande, l'heuristique prenant en compte le poids des sous-ensembles est
				systématiquement meilleure.

		\subsection{Méta-heuristiques}
			\subsubsection{Opérateurs de voisinage}
				\paragraph*{}
					Nous avons besoin d'opérateurs de voisinage pour implémenter certaines méta-heuristiques.
					C'est le cas des méthodes que nous avons implémenté.
					Étant donné notre représentation binaire, nous avons fait le choix d'implémenter des
					opérateurs de voisinage qui changent simplement la valeur d'un bit pour obtenir une solution voisine~:
					\begin{itemize}
						\item \emph{uniform\_flip\_bit}: choisi un bit au hasard uniformément et change sa valeur.
							En revanche, si, par exemple, on a beaucoup plus de 0 que de 1, on a aussi beaucoup plus de chance de
							passer un 0 à 1.
						\item \emph{flip\_bit}: choisi avec une chance sur deux de changer la valeur d'un bit 0 ou 1.
					\end{itemize}
					Dans les deux cas, on recommance l'opération jusqu'à avoir une solution valide.

			\subsubsection{Recuit simulé}\label{sec:sim-annealing}
				\paragraph*{}
					Nous avons implémenté le recuit simulé standard~: à chaque itération on applique un opérateur de voisinage.
					Si cela améliore la solution, on conserve la solution, sinon on on accepte avec une probalité \(e^{\frac{-\Delta}{T}}\) avec \(\Delta\) la différence de coût
					entre la solution courante et la nouvelle solution et \(T\) la température courante.
					Nous faisons varier la température uniformément entre deux bornes au fil des itérations.

			\subsubsection{Algorithme génétique}
				\paragraph*{}
					Nous avons également implémenté un algorithme génétique qui utilise une sélection sélection par rang ainsi qu'une recherche locale
					à l'aide du recuit simulé présenté dans la Section~\ref{sec:sim-annealing}.
					Les paramètres sont \(p\) la taille de la population, \(t_r\) le taux de remplacement, \(t_m\) le taux de mutations,
					\(t_l\) le taux de recherche locale, \(r_{iter}\) le nombre d'itérations du recuit simulé, \(r_{tmp}\) la température initiale du recuit simulé,
					\(r_{fin}\) la température finale du recuit simulé.

					\begin{itemize}
						\item On commence par générer une population \(P\) de \(n\) individus aléatoirement.
						\item À chaque itération (ou génération):
							\begin{itemize}
								\item On trie les individus par coût décroissant. L'indice d'un individu correspond alors à son rang.
								\item On supprime les individus on doublon pour favoriser l'exploration. On note \(D\) le nombre de doublons supprimés.
								\item On créer \(n' = n \times t_r\) nouveaux individus à partir de deux parents à chaque fois.
									Les deux parents sont sélectionnés selon leur rang. On applique un opérateur de croisement
									pour obtenir une troisième solution. Cette solution a une probabilité \(t_l\) de subir une
									recherche locale par recuit simulé avec les paramètres \(r_{tmp}\) et \(r_{fin}\) et une probabilité
									\(t_m\) de subir une mutation par opérateur de voisinage.
								\item Les \(n'\) nouveaux individus remplacement les \(n' - D\) pire individus de la génération précédente.
							\end{itemize}
						\item On élémine des sous-ensembles de la meilleure solution trouvée en lançant la procédure vorace pondérée limitée
							aux sous-ensembles choisis par la solution.
						\item On retourne cette solution.
					\end{itemize}

					L'opérateur de croisement implémenté consiste à résoudre le problème de façon vorace à l'aide de la méthode vorace pondérée
					sur les sous-ensembles choisis par les deux solutions à croiser uniquement.

		\subsection{Analyse des méthodes approchées appliquées aux problèmes d'OR-Library}
			\paragraph*{}
				Pour évaluer les méthodes approchées dévelppées, nous avons utiliser les problèmes d'OR-Library.
				Plus précisément, nous avons pris une instance de chaque groupe dont nous connaissons la solution optimale~: A.1, B.1, C.1., D.1, 4.1, 5.1.
				De plus, nous avons également pris une instance d'un groupe dont nous ne connaissons pas la solution optimale~: 6.1.

			\paragraph*{}
				Pour l'ajustement des paramètres des méta-heuristiques nous avons décidé d'utiliser un logiciel d'optimisation par boîte noire.
				Pour cela, il faut transformer le programme en boîte noire.
				C'est-à-dire que nous avons fait une version qui prend en argument les paramètres que l'on souhaite déterminer, et
				qui retourne seulement le résultat de la fonction objectif.

			\paragraph*{}
				Les paramètres que l'on souhaite optimiser sont~:
				\begin{itemize}
					\item Pour le recuit simulé~:
						\begin{itemize}
							\item la température initiale
							\item la température finale
						\end{itemize}
					\item Pour l'algorithme génétique~:
						\begin{itemize}
							\item la taille de la population
							\item le taux de remplacement
							\item le taux de mutations
							\item le taux de recherche locale par recuit simulé
							\item le nombre d'itérations par recuit simulé
							\item la température initiale du recuit simulé
						\end{itemize}
				\end{itemize}

			\paragraph*{}
				Nous utilisons l'algorithme de Mesh Adaptive Directed Search avec le logiciel NOMAD\footnote{{https://sourceforge.net/projects/nomad-bb-opt/}}
				pour chacun des problèmes proposés.
				Il faut noter que NOMAD fonctionne autant avec des boîtes noires déterministes qu'avec des boite non-déterministes.
				Il nous semble donc bien adapté à la situation.

			\paragraph*{}
				Pour l'algorithme génétique, nous avons configuré NOMAD pour effectuer 60 évaluations de boîte noire maximum.
				L'algorithme génétique est réglé pour s'arrêter au bout d'une minute d'exécution (il effectue autant de générations
				que possible en une minute).
				Le recuit simulé à été réglé pour effectuer 50 millions d'itérations (environ 3 minutes d'exécution) lors de 20 évaluations de boîte noire (toujours au maximum). La solution de départ du recuit est générée aléatoirement.

			\paragraph*{}
				La Table~\ref{table:approached-solutions} montre les résultats obtenus.
				On constate que le recuit simulé seul n'est pas nécessairement plus efficace que la simple procédure vorace pondérée (mais nécessite un temps
				de calcul bien plus conséquent). En revanche, l'algorithme génétique permet d'obtenir des solutions de qualité bien plus élevée en moins de temps que
				le recuit simulé. Toutes les solutions obtenues par l'algorithme génétique ont un pourcentage de différence à l'optimale de moins de \(5\%\).

			\begin{table}[H]
				\centering
				\input{tables/orlibrary_scp_approached}
				\caption{Coût des solutions obtenues à l'aide des méthodes approchées. La valeur entre parenthèses est le pourcentage de différence à l'optimale calculé
					à partir du ratio \(\frac{r-opt}{opt}\) avec \(r\) le meilleur coût obtenu et \(opt\) le coût optimal.}
				\label{table:approached-solutions}
			\end{table}

			\paragraph*{}
				La Table~\ref{table:genetic-parameters} contient les paramètres ayant permis d'obtenir la ou les solutions de meilleur coût avec l'algorithme génétique.
				La Table~\ref{table:annealing-parameters} contient ceux du recuit simulé.
				Il ne semble pas y avoir d'ensemble de paramètres excellents dans tous les cas et il nous semble difficile
				de trouver une corrélation entre la structure du problème et les meilleurs paramètres trouvés.

			\begin{table}[H]
				\centering
				\footnotesize
				\input{tables/genetic_parameters}
				\caption{Les paramètres qui nous ont permis d'obtenir la meilleure solution avec l'algorithme génétique.
					Le format est: (\(p\) \(t_r\) \(t_m\) \(t_l\) \(r_{iter}\) \(r_{tmp}\)) avec \(p\) = taille de la population,
					\(t_r\) = taux de remplacement, \(t_m\) = taux de mutations, \(t_l\) = taux de recherche locale, \(r_{iter}\) =
					le nombre d'itérations du recuit simulé, \(r_{tmp}\) = température initiale du recuit simulé.}
				\label{table:genetic-parameters}
			\end{table}

			\begin{table}[H]
				\centering
				\input{tables/simulated_annealing_parameters}
				\caption{Les paramètres qui nous ont permis d'obtenir la meilleure solution avec le recuit simulé.
					Le format est: (\(t_{init}\) \(t_{final}\)) avec \(t_{init}\) = température initiale et
					\(t_{final}\) = température finale}
				\label{table:annealing-parameters}
			\end{table}
	\section{Conclusion}
		\paragraph*{}
			Comme on pouvait s'y attendre d'un problème NP complet,
			il est très difficile de résoudre des problèmes en moins d'une heure, dans notre cas au delà d'une
			taille d'environ 35, à l'aide de la recherche exhaustive.
			La méthode exacte basée sur un branch-and-bound permet de pousser les limites considérablement.
			Cependant, selon la configuration du problème, un problème de taille \(n\) peut très bien prendre plus d'une heure tandis
			qu'un problème de taille \(n+1\) prendra seulement une minute.
			Nous n'avons en fait pas plus de garanties sur le temps d'exécution qu'avec la méthode exhaustive.
			Pour résoudre des problèmes de grande taille en un temps raisonnable, il est nécessaire de passer par des méthodes
			approchées. En utilisant des méthode voraces, il nous est possible de construire une solution faisable
			très rapidement en un temps polynomial. La méthode vorace pondérée présentée permet d'obtenir des résultats
			relativement bons comparativement au temps nécessaire. Nous n'avons pas eu plus de \(25\%\) de différence à l'optimale
			dans les problèmes testés. D'autre part, tel que nous l'avons implémenté, le recuit simulé ne nous a pas permis
			d'obtenir avec certitude des résultats vraiment meilleurs que la méthode vorace pondérée.
			Cependant, la méthode génétique implémentée par la suite utilisant le recuit simulé (algorithme mimétique)
			nous a permis d'obtenir des résultats de très bonne qualité (moins de \(5\%\) de différence à l'optimale sur
			les problèmes considérés) en un temps raisonnable (moins d'une minute). Mais le grand nombre de paramètres à
			choisir est une difficulté non négligable. Nous avons utilisé un logiciel d'optimisation de boîte noire pour nous aider
			à trouver de bons paramètres, mais cela prend alors plus de temps, et nous n'avons pas pu trouver de paramètres qui sont
			bons dans tous les cas. Les paramètres obtimaux semblent varier grandement selon le problème considéré.
	\newpage\printbibliography[heading=bibintoc]{}
\end{document}

% Problem set        Files
% 4                  scp41, ..., scp410
% 5                  scp51, ..., scp510
% 6                  scp61, ..., scp65
% A                  scpa1, ..., scpa5
% B                  scpb1, ..., scpb5
% C                  scpc1, ..., scpc5
% D                  scpd1, ..., scpd5
% E                  scpe1, ..., scpe5

% Problem set        Files
% E                  scpnre1, ..., scpnre5
% F                  scpnrf1, ..., scpnrf5
% G                  scpnrg1, ..., scpnrg5
% H                  scpnrh1, ..., scpnrh5
